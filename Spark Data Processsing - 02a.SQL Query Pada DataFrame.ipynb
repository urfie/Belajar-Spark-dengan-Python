{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMI+b+J1kZR4fUKB13qTg3M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["SQL adalah salah satu bahasa populer untuk pemrosesan dan analisis data. Spark mendukung SQL untuk memproses DataFrame.\n","\n","Kita akan menggunakan data yang sama dengan yg digunakan pada bab eksplorasi DataFrame.\n"],"metadata":{"id":"vJbZRSkkTnAw"}},{"cell_type":"code","metadata":{"id":"xyqGzlLOuBpV"},"source":["%pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJDNbI5gti9B"},"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inisialisasi spark session untuk berinteraksi dengan Spark cluster"],"metadata":{"id":"HgEv2iNhTitC"}},{"cell_type":"code","metadata":{"id":"Hnxed8rpt5Xs"},"source":["spark = SparkSession.builder.appName('DataFrame Basics').getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download dataset"],"metadata":{"id":"iHGCn-voUTzn"}},{"cell_type":"code","source":["!wget https://www.dropbox.com/s/4ozzaa2yvy2kkza/indonesia2013-2015.csv"],"metadata":{"id":"PQTqnwezHtSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load ke dataframe"],"metadata":{"id":"-vEl7qY6UV4Y"}},{"cell_type":"code","source":["df = spark.read.csv(\"indonesia2013-2015.csv\", header=True, inferSchema=True)"],"metadata":{"id":"ArREYTQWHy3J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sebelum menggunakan SQL, kita perlu membuat temporary table dari dataframe yang akan kita olah.\n","\n","Gunakan fungsi `createOrReplaceTempView(nama_tabel)` pada dataframe tersebut."],"metadata":{"id":"lvgyM2iKUYS9"}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"penduduk\")"],"metadata":{"id":"29qI_YUJnF_N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Selanjutnya kita bisa menggunakan nama tabel yang sudah kita definisikan dalam SQL statement.\n","\n","Untuk mengeksekusi SQL statement, kita gunakan fungsi `sql(sqlstatement)` pada spark session."],"metadata":{"id":"xuijaPcEUz3g"}},{"cell_type":"code","source":["spark.sql(\"select count(*) from penduduk\").show()"],"metadata":{"id":"mEZWLQbtnIHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark.sql(\"select * from penduduk limit 5\").show()"],"metadata":{"id":"6fLoE47EnMtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark.sql(\"\"\"select *,\n","     CASE\n","      WHEN timezone == 'WIT' THEN 1\n","      WHEN timezone == 'WITA' THEN 2\n","      ELSE 3\n","     END as kode_timezone\n","     from penduduk limit 5\"\"\").show()"],"metadata":{"id":"IkuuNd0ynWll"},"execution_count":null,"outputs":[]}]}