{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"199UVfubvTdbnwCi3w1vhy7UZAblU0kNE","timestamp":1675862460099},{"file_id":"1UC78SbP8xVTGKSjNejDaeArd4G6O3N-c","timestamp":1598515071797}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["Dalam latihan ini kita akan melakukan pemrosesan data menggunakan data sales.\n","\n","Format data adalah sbb :\n","- seq\n","- product_name\n","- qty\n","- date\n","- Size\n","- length\n","- cust_no\n","\n","Kita akan menggunakan dataset yang mengandung bad records, atau baris yang formatnya tidak valid."],"metadata":{"id":"UPYFNMzcRJtx"}},{"cell_type":"code","metadata":{"id":"Z2YaQG6_54SX"},"source":["!wget https://www.dropbox.com/s/gktr38tfatnhcf8/salesdata_bad.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_w_sWv3QtjN"},"source":["# Mempersiapkan environment"]},{"cell_type":"markdown","metadata":{"id":"1wtBVhZeQtjR"},"source":["##a. Install pyspark package\n","Untuk menjalankan notebook ini di Google colab, langkah pertama yang perlu dilakukan adalah menginstall package `pyspark`, karena package tersebut tidak disertakan secara default.\n","\n","Langkah ini perlu dilakukan setiap membuka session/notebook baru.\n","\n","Instalasi kita lakukan dengan perintah `pip`"]},{"cell_type":"code","metadata":{"id":"xyqGzlLOuBpV"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ddekh7YVnsI"},"source":["\n","##b. Create spark session"]},{"cell_type":"code","metadata":{"id":"qJDNbI5gti9B"},"source":["import pyspark\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hnxed8rpt5Xs"},"source":["spark = SparkSession.builder.appName('Data Preprocessing').getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVpGOEFE5flN"},"source":["# 1.Loading Data : Menangani *bad data*\n","\n"]},{"cell_type":"markdown","source":["Cek format file yang sudah kita download dengan perintah `head`"],"metadata":{"id":"VHf8dBuiVYA1"}},{"cell_type":"code","source":["!head salesdata_bad.csv"],"metadata":{"id":"ffV638DBVTqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmPZ03nAolQ1"},"source":["!wc -l salesdata_bad.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load ke dataframe, karena mengandung header, kita set `header = True`"],"metadata":{"id":"WEPTW-NzXHaV"}},{"cell_type":"code","metadata":{"id":"CFosU6ejr8_k"},"source":["dfu = spark.read.csv(\"salesdata_bad.csv\", header=True)\n","dfu.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tampilkan skema dataframe. Secara default seluruh kolom dibaca sebagai string."],"metadata":{"id":"WlEWzAU0YANC"}},{"cell_type":"code","source":["dfu.printSchema()"],"metadata":{"id":"nBnJHt1DYDSH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h21X1CJ26JMe"},"source":["Tampilkan summary statistik"]},{"cell_type":"code","metadata":{"id":"EG5PmO81AVwM"},"source":["dfu.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmF_zHdOAgCy"},"source":["Dari statistik di atas kita bisa melihat bahwa terdapat record dengan nilai `product_name` dan/atau `qty` yang null\n","\n","Kita juga bisa melihat adanya nilai non numerik pada kolom `qty`"]},{"cell_type":"markdown","metadata":{"id":"cmFI-i_2NvVy"},"source":["##1.1 Mendefinisikan Skema\n","\n","Pada umumnya data yang kita hadapi mengandung record-record invalid.\n","\n","Untuk menanganinya pada saat loading, kita bisa mendefinisikan skema untuk dataframe yang akan kita gunakan.\n","\n","Kita akan membutuhkan `StructType, StructField, IntegerType, StringType, DateType` dari library `pyspark.sql.types`."]},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"],"metadata":{"id":"gmxy84dKtu9l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"camqT2Kjtwwq"}},{"cell_type":"code","metadata":{"id":"SclITPw3FtpV"},"source":["salesSchema = StructType([\n","    StructField(\"seq\", IntegerType(), True),\n","    StructField(\"product_name\", StringType(), True),\n","    StructField(\"qty\", IntegerType(), True),\n","    StructField(\"salesdate\", StringType(), True),\n","    StructField(\"size\", StringType(), True),\n","    StructField(\"length\", StringType(), True),\n","    StructField(\"cust_no\", IntegerType(), True)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ru9vpdnOtEwb"},"source":["##1.2 Menangani data reject/malformed\n","\n","Spark menyediakan 3 mode setting untuk loading file csv dan json.\n","\n","1. `PERMISSIVE` : load record yang invalid, selama null constrain terpenuhi\n","2. `DROPMALFORMED` : ignore/skip record yang tidak sesuai formatnya\n","3. `FAILFAST` : throw exception ketika menemukan record yang invalid\n","\n","Secara default `mode` diset ke `PERMISSIVE`."]},{"cell_type":"markdown","source":["Ketika menggunakan mode ``PERMISSIVE``, semua data diload ke dataframe, dan kolom yang tidak valid diset ke NULL, karena skema yang digunakan nullability-nya diset `True`.\n"],"metadata":{"id":"KYMJpStVBhx_"}},{"cell_type":"code","metadata":{"id":"WtyyIRHTRJdP"},"source":["dfu_p = spark.read.csv(\"salesdata_bad.csv\", header=True, schema=salesSchema)\n","print(\"Loaded records: \", dfu_p.count())\n","dfu_p.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ioZDagcKa3d"},"source":["Ketika mode diset`DROPMALFORMED`, record dengan format invalid tidak di-load.\n"]},{"cell_type":"code","metadata":{"id":"Jea_ZYGcVmDr"},"source":["dfu_d = spark.read.csv(\"salesdata_bad.csv\", header=True, schema=salesSchema, mode=\"DROPMALFORMED\")\n","print(\"Loaded records: \", dfu_d.count())\n","dfu_d.describe().show()\n","dfu_d.count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dfu_d.show(100)"],"metadata":{"id":"eRBnNJuN7mG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRiKertF7t5G"},"source":["Ketika `mode` diset `FAILFAST`, terjadi error karena ada exception. Kita bisa memilih apa yang akan dilakukan dengan melakukan exception handling."]},{"cell_type":"code","metadata":{"id":"P6ExfeNyqb8g"},"source":["dfu_d = spark.read.csv(\"salesdata_bad.csv\", header=True, schema=salesSchema, mode=\"FAILFAST\")\n","print(\"Loaded records: \", dfu_d.count())\n","dfu_d.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWkUZ6s59DzD"},"source":["#2.Menangani NULL"]},{"cell_type":"markdown","metadata":{"id":"_WZ7VUtY9R1x"},"source":["##2.1 Menampilkan nilai NULL\n","\n","Kita dapat memeriksa record mana yang memiliki null di salah satu kolom dengan memilih record menggunakan fungsi `isNull()` pada kolom tersebut.\n","\n","Di sini terlihat perbedaan antara hasil loading menggunakan mode `PERMISIVE` dan `DROPMALFORMED`"]},{"cell_type":"code","metadata":{"id":"MdN_MFOBIeQc"},"source":["print(\"PERMISSIVE with seq = null\")\n","dfu_p[dfu_p[\"seq\"].isNull()].show()\n","print(\"DROPMALFORMED with seq = null\")\n","#dfu_d[dfu_d[\"seq\"].isNull()].show()\n","\n","print(\"PERMISSIVE with product_name = null\")\n","dfu_p[dfu_p[\"product_name\"].isNull()].show()\n","print(\"DROPMALFORMED with product_name = null\")\n","#dfu_d[dfu_d[\"product_name\"].isNull()].show()\n","\n","print(\"PERMISSIVE with qty = null\")\n","dfu_p[dfu_p[\"qty\"].isNull()].show()\n","print(\"DROPMALFORMED with qty = null\")\n","#dfu_d[dfu_d[\"qty\"].isNull()].show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVMhWx5_NtON"},"source":["Pertanyaan selanjutnya adalah: Apa yang harus dilakukan dengan nilai NULL?\n","\n","Ada beberapa pilihan untuk menangani NULL, misalnya menghapus record, mengupdate nilai NULL ke suatu nilai konstan, atau berdasar kolom-kolom lainnya.\n"]},{"cell_type":"markdown","metadata":{"id":"NgG_wLnI9XEQ"},"source":["##2.2 Menghapus record dengan nilai NULL\n","\n","Kita dapat menghapus record yang mengandung nilai null dengan menggunakan `dropna(how, thresh, subset)`.\n","\n","Ada beberapa opsi yang dapat digunakan :\n","- hapus record dengan nilai NULL di kolom mana saja --> set `how = `**any**\n","- hapus record dengan semua kolom = null --> atur `how = `**all**\n","- hapus record dengan jumlah kolom yg null < n --> atur `thresh = (jumlah kolom - n)`\n","- hapus record dengan nilai NULL di kolom tertentu --> masukkan nama kolom di `subset`\n","\n","\n","Misalnya kita ingin menghapus semua record dengan nilai null di kolom manapun (kita akan menggunakan data yang dimuat dengan mode `PERMISIVE`)\n"]},{"cell_type":"code","metadata":{"id":"koox2JlEVy-q"},"source":["print(dfu_p.count())\n","dfu_p.dropna().count()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EdM-mp_lalpd"},"source":["Jika kita ingin memperbolehkan 1 nilai null per record, kita bisa gunakan parameter `thresh`"]},{"cell_type":"code","metadata":{"id":"s_fuq2Gcal_p"},"source":["dfu_p.dropna(thresh=6).count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NG0Is0c9bovI"},"source":["Untuk menghapus record dengan kolom `qty` = null"]},{"cell_type":"code","metadata":{"id":"s_WoWLm3buaU"},"source":["dfu_p = dfu_p.dropna(subset=[\"qty\"])\n","dfu_p.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_bepiSEJkW9"},"source":["##2.3 Mengganti NULL dengan default value\n","\n","Untuk mengganti nilai NULL dengan nilai tertentu, gunakan `fillna(value, subset)`.\n","\n","Misalnya untuk mengganti nilai null di kolom `qty` dengan 0 :"]},{"cell_type":"code","metadata":{"id":"tMzzzL8VfkA8"},"source":["df_1 = dfu_p.fillna(0,[\"qty\"])\n","df_1[df_1[\"qty\"].isNull()].show()\n","df_1.describe(\"qty\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pS6ES2SQg_LF"},"source":["##WRAPPING UP 1\n","============================\n","\n","Try this for exercise :\n","- Drop records with ``seq`` and/or ``salesdate`` is null\n","- Set null ``qty`` to 0\n","- Set null ``product_name`` to \"UNKNOWN\"\n","\n","Coba kerjakan latihan berikut ini:\n","- Hapus record dengan kolom `seq` dan/atau `salesdate` null\n","- Replace `product_name` yang null menjadi **UNKNOWN**"]},{"cell_type":"markdown","metadata":{"id":"gaTaQUikCi-9"},"source":["#3.Menangani data duplikat\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i4_v7Q_JITZN"},"source":["###3.1 Menghapus data duplikat\n","\n","\n","Untuk menghapus record duplikat, gunakan ` dropDuplicates(subset)` atau `drop_duplicates(subset)`.\n","\n","\n","Jika kita tidak menentukan nama kolom di parameter `subset`, record yang dihapus adalah record yang semua kolomnya identik.\n","\n","Dalam file kita, terdapat record seperti berikut ini:\n","  <br>``seq, product_name , qty, salesdate``\n","  <br>``88 ,Kemeja Bergaris Ungu, 1 ,2019/01/30``\n","  <br>``88 ,Kemeja Bergaris Ungu, 1 ,2019/01/30``\n","  <br>``88 ,Kemeja Hitam , 2 ,2019/01/30``\n","\n","\n","\n","Tanpa menentukan nama kolom, hanya record kedua yang akan dibuang."]},{"cell_type":"code","source":["df_1[df_1.seq == 88].show()"],"metadata":{"id":"LQEcAKdgZCnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1N6f7oh96dn0"},"source":["df_1.dropDuplicates()[df_1.seq == 88].show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Jika kita menentukan bahwa kolom `seq` harus unik, maka record kedua dan ketiga akan dihapus."],"metadata":{"id":"eeNcyf2gLvwQ"}},{"cell_type":"code","metadata":{"id":"3yfYXgRitIqR"},"source":["df_1 = df_1.dropDuplicates([\"seq\"])\n","df_1[df_1.seq == 88].show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C2v9Ho0-CZf8"},"source":["#4.Further Cleaning : Reformatting Columns\n","\n","In this section we will use :\n","- ``WithColumn()`` function\n","- Dataframe operation with pyspark built in functions\n","- Conditional operation with ``when()`` and ``when().otherwise()``"]},{"cell_type":"markdown","metadata":{"id":"Cq13n4DbmHpy"},"source":["DataFrame (and RDD) is immutable, so if we want to update/reformat a column, we have to add the formatted value as a new column and drop the original later\n","\n","We can add new column based on existing columns's value by using ``WithColumn()`` function.\n"]},{"cell_type":"markdown","metadata":{"id":"RJuuJSxcIggo"},"source":["##4.1 Standarisasi nilai\n","\n","Kita akan bersihkan kolom `size`. Pertama, kita tampilkan nilai unik kolom tersebut"]},{"cell_type":"code","metadata":{"id":"qOc2Rj3oIjXh"},"source":["df_1.select(\"size\").distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8V2bJSapAIJM"},"source":["Kita bisa melihat ada beberapa nilai yang tidak standar seperti ``Small``, ``xs``, dan ``m``. Mari ubah nilai tidak standar menjadi 'extra small', 'small', 'medium', dan 'big'.\n","\n","Kita bisa melakukannya dalam 3 langkah:\n","- ubah menjadi huruf kecil / lowercase\n","- hapus karakter non-alfabetikal\n","- ganti *xs* dan *m* masing-masing menjadi 'extra small' dan 'medium'"]},{"cell_type":"markdown","source":["###1.Ubah ke lowercase"],"metadata":{"id":"kss3q9GyMs05"}},{"cell_type":"code","metadata":{"id":"PUkRWPw9QB3L"},"source":["df_1_lower = df_1.withColumn(\"size_lower\", F.lower(\"size\"))\n","df_1_lower.select(\"size_lower\").distinct().show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6xCm4aVhhdB"},"source":["###2.Hapus karakter non-alfabet\n","\n","Kita akan gunakan regex (regular expression) untuk menghapus karakter nonalfabet, dengan fungsi `regexp_replace`.\n","Pola regex untuk alfabet huruf kecil adalah `[^a-z]` (karena kita sudah menerapkan `lower` sebelumnya)."]},{"cell_type":"code","metadata":{"id":"NXlEe7hdhovp"},"source":["df_1_regex = df_1_lower.withColumn(\n","        'size_regex',\n","        F.regexp_replace('size_lower', r'([^a-z])', ''))\n","df_1_regex.select('size_regex').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPi6oBQz-JRT"},"source":["###3.Ubah nilai berdasar aturan tertentu\n","\n","Untuk menerapkan aturan/kondisi tertentu, gunakan fungsi `when()` dan `otherwise()`"]},{"cell_type":"code","metadata":{"id":"ODZaGOPSiLAh"},"source":["df_1_clean = df_1_regex.withColumn('size_clean',\n","                      F.when(F.col('size_regex') == \"xs\" , \"extra small\")\n","                        .when(F.col('size_regex') == \"m\", \"medium\")\n","                          .otherwise(F.col('size_regex')))\n","\n","df_1_clean.select(\"size_clean\").distinct().show()\n","df_1_clean.show(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E6tFxyJyIoOG"},"source":["##4.2 Date Formatting"]},{"cell_type":"markdown","metadata":{"id":"he7e4WVwthiC"},"source":["Kolom salesdate masih berformat string. Kita bisa mengubahnya menjadi format Date dengan menggunakan fungsi `to_date()`.\n","\n","*Catatan : Kita juga bisa menentukan format Date ini ketika loading.*"]},{"cell_type":"code","metadata":{"id":"_B8ZXZbctmyW"},"source":["df_date = df_1_clean.withColumn(\"salesdate\", F.to_date(\"salesdate\",'yyyy/MM/dd'))\n","\n","df_date.show()\n","df_date.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tu42VJbX9QYi"},"source":["##WRAPPING UP 2\n","=====================\n","\n","Lakukan latihan berikut ini :\n","- Remove duplicate di kolom `seq`\n","- Bersihkan kolom `length`, replace null menjadi '-' and nilai lain ke dalam salah satu nilai ini : `long, midi, short`\n"]},{"cell_type":"markdown","metadata":{"id":"s98ktXYQEzq-"},"source":["#5.User Defined Function"]},{"cell_type":"markdown","metadata":{"id":"ZE4iUMBCFrTb"},"source":["UDF relatif sulit untuk dioptimasi, sehingga sebisa mungkin kita gunakan fungsi yang sudah disediakan oleh spark.\n","\n","Sebelum memutuskan untuk menggunakan UDF, cek terlebih dahulu ke https://spark.apache.org/docs/latest/sql-ref.html untuk memastikan bahwa fungsi yang kita perlukan memang belum tersedia.\n"]},{"cell_type":"markdown","source":["Untuk latihan ini, kita akan memproses kolom `product_name`."],"metadata":{"id":"0hVeDPxhRMQG"}},{"cell_type":"code","metadata":{"id":"zRSDcWqQFOh7"},"source":["df_date.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPwdWD8zmYlO"},"source":["Kita akan ubah huruf pertama dari setiap kata menjadi uppercase, dan sisanya menjadi huruf kecil, dengan menggunakan UDF.\n","\n","Pertama-tama kita buat fungsi python biasa."]},{"cell_type":"code","metadata":{"id":"p0kO3TR5mZZe"},"source":["def convertCase(str):\n","    myStr = \"\"\n","    words = []\n","    if str: words = str.split(\" \")\n","    for x in words:\n","       myStr = myStr + x[0:1].upper() + x[1:len(x)].lower() + \" \"\n","    return myStr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAvQAEnPmyzS"},"source":["Selanjutnya *wrap* fungsi tersebut dengan `pyspark.sql.functions.udf()`"]},{"cell_type":"code","metadata":{"id":"cGaHr97mmzWn"},"source":["from pyspark.sql.functions import udf\n","convertUDF = udf(lambda x: convertCase(x),StringType())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhskLTrnm56U"},"source":["Terapkan ke dataframe, dan tampilkan hasilnya"]},{"cell_type":"code","metadata":{"id":"M7oWHZ4Rm6lG"},"source":["df_date.select(\"seq\", \\\n","    convertUDF(\"product_name\").alias(\"ProductName\") ) \\\n","   .show(5, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVuPYH1tw6Rk"},"source":["For exercise : gunakan UDF dan `withColumn()` to mengupdate kolom product_name."]},{"cell_type":"code","metadata":{"id":"6gXlrlzDxEoK"},"source":["df_clean_2 = df_date.withColumn(\"product_name\", convertUDF(\"product_name\"))\n","df_clean_2.show(5, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l3zq4dYnKIt"},"source":["#6.Enrichment - Joining multiple DataFrame"]},{"cell_type":"markdown","metadata":{"id":"N792BfX9ubmL"},"source":["Kita akan melakukan data enrichment dengan menggabungkan dataframe sales dengan data customer."]},{"cell_type":"code","metadata":{"id":"oS3P8sQQNUbK"},"source":["!wget https://www.dropbox.com/s/hsb6lfkni466hpz/customers.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cek file yang kita download"],"metadata":{"id":"X2dVtQq8Spni"}},{"cell_type":"code","source":["!head customers.csv"],"metadata":{"id":"77GPrwr8StmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpZ12HBdNWsY"},"source":["##6.1 Loading data referensi"]},{"cell_type":"code","metadata":{"id":"0STvW-pI0GrL"},"source":["df_cust = spark.read.csv(\"customers.csv\",header=True,inferSchema=True)\n","df_cust.show(5, truncate=False)\n","\n","df_cust.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##6.2 Join Dataframe"],"metadata":{"id":"5PiiJOibTRKM"}},{"cell_type":"code","metadata":{"id":"AXbox2YQgndO"},"source":["df_joined = df_clean_2.join(df_cust,df_clean_2.cust_no ==  df_cust.seq, how = 'left')\n","df_joined.show(5, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMgE6Rd6N9ul"},"source":["df_joined.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Update history\n","\n","2302.1555\n","* _Translate dan lengkapi penjelasan_\n","* _Rearrange sections_\n","* _Code cleanup_\n"],"metadata":{"id":"F4iptlYpYtlg"}}]}