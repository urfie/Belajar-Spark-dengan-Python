{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YNrLcxafFLkqoUTW4bxROxiwvLZJfJ-B","timestamp":1675321257077},{"file_id":"1UC78SbP8xVTGKSjNejDaeArd4G6O3N-c","timestamp":1598507463321}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VSqTiubIBpoG"},"source":["# Mempersiapkan environment"]},{"cell_type":"markdown","metadata":{"id":"8elK3IvmVTew"},"source":["##a. Install pyspark package\n","Untuk menjalankan notebook ini di Google colab, langkah pertama yang perlu dilakukan adalah menginstall package `pyspark`, karena package tersebut tidak disertakan secara default.\n","\n","Langkah ini perlu dilakukan setiap membuka session/notebook baru.\n","\n","Instalasi kita lakukan dengan perintah `pip`"]},{"cell_type":"code","metadata":{"id":"xyqGzlLOuBpV"},"source":["%pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ddekh7YVnsI"},"source":["\n","##b. Membuat spark session\n","\n","Import library yang akan digunakan."]},{"cell_type":"code","metadata":{"id":"qJDNbI5gti9B"},"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inisialisasi spark session untuk berinteraksi dengan Spark cluster"],"metadata":{"id":"HgEv2iNhTitC"}},{"cell_type":"code","metadata":{"id":"Hnxed8rpt5Xs"},"source":["spark = SparkSession.builder.appName('DataFrame Basics').getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aiotjcgbz9gv"},"source":["#1.Lab 02 - Membuat DataFrame"]},{"cell_type":"markdown","source":["DataFrame dapat dibuat dengan banyak cara, di antaranya :\n","- Dari python object, misalnya array/list, dictionary, pandas dataframe, dll\n","- Dari file : csv, json, dll\n","- Dari HDFS\n","- Dari RDD\n","- dll."],"metadata":{"id":"R4puMNX2WfRN"}},{"cell_type":"markdown","metadata":{"id":"dOf-4dxAgOsv"},"source":["##1.1. Create From Array"]},{"cell_type":"code","metadata":{"id":"ScgatWyqepcH"},"source":["mydata = (('DKI JAKARTA',15328),\n","('JAWA BARAT',1320),\n","('JAWA TENGAH',1030),\n","('DI YOGYAKARTA',1174),\n","('JAWA TIMUR',813),\n","('BANTEN',1237))\n","\n","df_from_array = spark.createDataFrame(mydata).toDF(\"province\", \"density\")\n","\n","df_from_array.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XYN1nYAiWx6u"},"source":["##1.2. Create from Pandas DataFrame"]},{"cell_type":"markdown","source":["Dowload file"],"metadata":{"id":"H4ksYmW7ZCXF"}},{"cell_type":"code","metadata":{"id":"FSEQn5OJ4vnK"},"source":["!wget https://www.dropbox.com/s/65mohvoyjshtqa9/penduduk2015.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create pandas dataframe dari file csv tersebut"],"metadata":{"id":"AzgGjBoCZEqP"}},{"cell_type":"code","metadata":{"id":"34owT-3uAGca"},"source":["import pandas as pd\n","\n","pddf = pd.read_csv('penduduk2015.csv')\n","pddf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ubah ke Spark dataframe"],"metadata":{"id":"3P8yoarCZL5o"}},{"cell_type":"code","metadata":{"id":"r7Hyq3iuO-RF"},"source":["df_from_pandas = spark.createDataFrame(pddf)\n","df_from_pandas.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKco8E9FgidD"},"source":["##1.3. Create from csv"]},{"cell_type":"markdown","source":["Kita juga bisa me-load langsung file csv tersebut ke Spark dataframe"],"metadata":{"id":"5IwfM7QBZRlS"}},{"cell_type":"code","metadata":{"id":"zQ3yUkvjlKqN"},"source":["df_from_csv = spark.read.csv(\"penduduk2015.csv\", header=True)\n","df_from_csv.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n83qkTxzrrma"},"source":["##1.4. Create from JSON"]},{"cell_type":"code","metadata":{"id":"i_NWvf1E52v2"},"source":["!wget https://www.dropbox.com/s/8wpyzukkmmn3t6g/penduduk2015.json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTcgb1HEevnh"},"source":["Tampilkan isi file dengan perintah `cat`"]},{"cell_type":"code","metadata":{"id":"3nI8eugSjORM"},"source":["!cat penduduk2015.json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Untuk membaca multiline JSON, set parameter `multiline` = True"],"metadata":{"id":"cmMxONptZmPU"}},{"cell_type":"code","metadata":{"id":"XhVBi_ndruRA"},"source":["dfj = spark.read.json(\"penduduk2015.json\", multiLine=True)\n","dfj.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzpuNpb8myW3"},"source":["#2.Lab 03 - Explorasi DataFrame\n","\n","Dalam latihan ini kita akan mencoba berbagai operasi pada Spark DataFrame untuk melakukan eksplorasi data.\n","\n","Kita akan menggunakan data kepadatan penduduk per propinsi."]},{"cell_type":"code","metadata":{"id":"-qO8foHlWexn"},"source":["!wget https://www.dropbox.com/s/4ozzaa2yvy2kkza/indonesia2013-2015.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Kita gunakan magic command untuk melihat ukuran dan isi file (karena file kita cukup kecil)."],"metadata":{"id":"3qD7ifpcl4ue"}},{"cell_type":"code","source":["%ls -al indonesia2013-2015.csv"],"metadata":{"id":"4YKCOaiwmKO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cat indonesia2013-2015.csv"],"metadata":{"id":"kEX7VuFZl498"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Karena data yang kita load sudah bersih, kita akan set inferSchema = True agar Spark menyesuaikan tipe kolom dengan datanya."],"metadata":{"id":"Iw-Y0fIMl29B"}},{"cell_type":"code","metadata":{"id":"4tQqD2VYm7ZR"},"source":["df = spark.read.csv(\"indonesia2013-2015.csv\",header=True,inferSchema=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vbwg-RH5m1JG"},"source":["###2.1 Melihat sekilas"]},{"cell_type":"markdown","metadata":{"id":"oTGwnupfKBMU"},"source":["####Menampilkan beberapa baris\n","\n","Biasanya kita menampilkan beberapa baris data untuk mengecek format dan konten dataframe yang kita buat.\n","\n","Untuk menampilkan beberapa baris dari dataframe, kita bisa gunakan perintah ``show(n)`` untuk menampilkan n baris pertama, atau ``first()`` untuk menampilkan 1 baris pertama saja."]},{"cell_type":"code","metadata":{"id":"y-ccShKgnGBw"},"source":["df.show(5)\n","df.first()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fX7rzHbOQRyv"},"source":["####Menampilkan jumlah kolom"]},{"cell_type":"code","metadata":{"id":"CbUMZTJ8QSYF"},"source":["len(df.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkyOlEULP_g-"},"source":["####Menampilkan total records"]},{"cell_type":"code","metadata":{"id":"YtSRF5xFQBmc"},"source":["df.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99-M00F5nKdO"},"source":["####Menampilkan skema\n","\n","Untuk menampilkan skema dataframe, gunakan fungsi `printSchema()`\n","\n"]},{"cell_type":"code","metadata":{"id":"WOlh-X3nnL62"},"source":["df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_rKrAAenlC0"},"source":["Akses atribut `columns` untuk menampilkan list nama kolom\n"]},{"cell_type":"code","metadata":{"id":"y8el4g04nnII"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Akses atribut `dtypes` untuk menampilkan list nama kolom beserta data type masing-masing kolom tersebut"],"metadata":{"id":"juHaBiLq_ldO"}},{"cell_type":"code","metadata":{"id":"49CkcSSNnuRH"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OpPxH6tlnY5R"},"source":["####Menampilkan summary statistik\n","\n","Fungsi `describe()` digunakan untuk menampilkan summary statistik dari seluruh kolom.\n","\n","Jangan lupa memanggil fungsi `show()` untuk menampilkan hasilnya."]},{"cell_type":"code","metadata":{"id":"S_WjXzh-ncSu"},"source":["df.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWykQ_0KnpLT"},"source":["Untuk menampilkan statistik dari salah satu kolom saja, gunakan nama kolom yang akan ditampilkan sebagai parameter. Misalnya `describe('column1')`"]},{"cell_type":"code","metadata":{"id":"feq2JOTOn0dq"},"source":["df.describe(\"density\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYDiKb4Rn_0c"},"source":["###2.2 Filtering\n","\n"]},{"cell_type":"markdown","source":["Kita dapat melakukan filtering terhadap spark dataframe, berdasar kolom atau baris"],"metadata":{"id":"Y3BTu53eBAqp"}},{"cell_type":"markdown","metadata":{"id":"-om-OrZquQQo"},"source":["####Memilih kolom tertentu\n","\n","Untuk menampilkan kolom tertentu, digunakan fungsi `select('nama_kolom')`\n"]},{"cell_type":"code","metadata":{"id":"QdQzWlmvoJXv"},"source":["df.select(\"year\").show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1x_c-b9pcWc"},"source":["Untuk memilih beberapa kolom, gunakan tanda koma sebagai pemisah"]},{"cell_type":"code","metadata":{"id":"SQbyEyDCpekb"},"source":["df.select(\"province\",\"density\").show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghJrZSCpoo7C"},"source":["####Memilih records / baris\n","\n","Untuk memilih baris dengan kondisi tertentu, gunakan fungsi `filter(<kondisi>)`\n","\n"]},{"cell_type":"code","metadata":{"id":"xwTmG_QyoQHM"},"source":["df.filter(df.density > 5000).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c4790g0r7GK"},"source":["Untuk menggunakan kondisi berupa operasi string, dapat digunakan fungsi-fungsi dari `pyspark.sql.Column` yang terkait string, misalnya `contains()`, `startswith()`, `endswith()`"]},{"cell_type":"code","metadata":{"id":"8VL5e4DFofWH"},"source":["df.filter(df.province.contains('TENGGARA')).show(5)\n","df.filter(df.province.startswith('SU')).show(5)\n","df.filter(df.province.endswith('BARAT')).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2u7x8thvs2yp"},"source":["Tersedia juga fungsi `like()` yang serupa dengan SQL statement *like*"]},{"cell_type":"code","metadata":{"id":"V3qDU-rYs7kc"},"source":["df.filter(df.province.like('SU%')).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWaHUzxHtSeg"},"source":["Atau dapat juga menggunakan regex, dengan fungsi `rlike()`"]},{"cell_type":"code","metadata":{"id":"OVjEkO-vtUtC"},"source":["df.filter(df.province.rlike('[A-Z]*TA$')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"114ZKh-PuArH"},"source":["Dapat juga menggunakan filter berdasar list, dengan fungsi `isin()`"]},{"cell_type":"code","metadata":{"id":"0FNg0NkIt-2g"},"source":["df.filter(df.timezone.isin('WIT','WITA')).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_zzv1_aYXBe"},"source":["Untuk menggunakan beberapa kondisi sekaligus, menggunakan tanda `&` untuk AND dan `|` untuk OR, dengan masing-masing kondisi dilingkupi tanda kurung `()`"]},{"cell_type":"code","metadata":{"id":"5dAZ7aDoYalg"},"source":["df.filter((df.timezone.isin('WIT','WITA')) & (df.year == 2013)).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZb03bk5QKwo"},"source":["###2.3 Unique value\n","\n","Untuk menampilkan nilai unik dari dataframe, digunakan fungsi `distinct()`.\n","\n","Nilai unik di sini adalah kombinasi nilai dari seluruh kolom."]},{"cell_type":"code","metadata":{"id":"sAIvwxbpQbj-"},"source":["df.distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_2VKvj1QnVc"},"source":["Untuk menampilkan nilai unik dari kolom tertentu, tulis nama kolom yang dimaksud sebagai parameter."]},{"cell_type":"code","metadata":{"id":"EjlP69w-QqaV"},"source":["df.select('timezone').distinct().show()\n","df.select('year','timezone').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Menghapus duplikasi data"],"metadata":{"id":"P3Uie0m1uVeJ"}},{"cell_type":"markdown","source":["Untuk menghapus record duplikat, gunakan `dropDuplicates(subset)` atau `drop_duplicates(subset)`."],"metadata":{"id":"65nP6e1lgTFE"}},{"cell_type":"code","source":["df.dropDuplicates(['province', 'timezone']).show(100)"],"metadata":{"id":"5feLEyAJuZPw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcLgEXz-PlvN"},"source":["###2.4 Agregasi"]},{"cell_type":"markdown","metadata":{"id":"R0kFRECNRqBL"},"source":["####Group by column\n","\n","Untuk mengelompokkan berdasar kolom, gunakan perintah `groupBy('nama_kolom')`\n","\n","Untuk mengelompokkan berdasar lebih dari 1 kolom, gunakan tanda koma sebagai pemisah nama kolom."]},{"cell_type":"code","source":["df.groupBy(\"timezone\")"],"metadata":{"id":"oVa4FSWws8Xo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupBy(\"timezone\",\"year\")"],"metadata":{"id":"IKsy1iphtWwe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perintah `groupBy()` menghasilkan obyek `GroupedData` yang belum bisa ditampilkan.\n","\n","Biasanya setelah pengelompokan, kita melakukan operasi sumarisasi data. Kita terapkan operasi tersebut pada objek hasil groupBy dengan memanggil fungsi yang dibutuhkan. Misalnya `count()` atau `max('namakolom')`"],"metadata":{"id":"3bzjnDEQtdiI"}},{"cell_type":"code","metadata":{"id":"B9aXQsIWFt3O"},"source":["df.groupBy('timezone').count().show()\n","df.groupBy('timezone').max('density').show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Kr85hEQJl8a"},"source":["Kita juga bisa menggunakan fungsi `agg()` untuk melakukan agregasi. Terutama jika kita ingin melakukan lebih dari 1 operasi agregat.\n","\n","Kita bisa menggunakan fungsi `alias()` untuk memberi nama kolom hasil agregasi."]},{"cell_type":"code","metadata":{"id":"_d2I29cpHV_Q"},"source":["df.groupBy(\"timezone\").agg(F.max('density')).show()\n","df.groupBy(\"timezone\").agg(F.avg('density').alias('avg_density'), \\\n","                           F.min('density').alias('min_density'), \\\n","                           F.max('density').alias('max_density')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cci7rwBA0XiD"},"source":["####Order By"]},{"cell_type":"code","metadata":{"id":"4LMTfZnTiZMI"},"source":["df.groupBy(\"timezone\") \\\n","  .mean(\"density\") \\\n","  .orderBy(\"timezone\",ascending=False).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JD-e2D1X0lKq"},"source":["####Agregasi dengan filter / kondisi"]},{"cell_type":"code","metadata":{"id":"RBy0oEPGhQAL"},"source":["df.groupBy(\"timezone\") \\\n","  .mean(\"density\") \\\n","  .where(df.timezone.contains('WIT')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtQRWTYttuXh"},"source":["####Filter hasil agregat (SQL stat **HAVING**)\n","\n","Untuk memfilter berdasar hasil agregasi (semacam perintah **HAVING** di SQL), lakukan dalam 2 langkah.\n","1. Lakukan `groupBy` + `agg` dan beri nama kolom hasil agregat dengan `alias`\n","2. gunakan fungsi `filter(kondisi)` pada kolom hasil agregasi."]},{"cell_type":"code","source":["df_agg = df.groupBy(\"timezone\", \"province\") \\\n","  .agg(F.avg(\"density\").alias(\"avg_density\")) \\\n","  .where(df.timezone.contains('WIT'))\n","\n","df_agg.filter(df_agg.avg_density > 50).show()"],"metadata":{"id":"B9VnDO8Dw3AO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###2.5 Transformasi DataFrame"],"metadata":{"id":"t_PLmohQyBSM"}},{"cell_type":"markdown","source":["####Kolom baru berupa nilai konstan\n","\n","Untuk menambahkan kolom baru ke dalam dataframe, kita bisa menggunakan perintah `withColumn()`\n","\n","Sedangkan untuk menambahkan sebuah nilai konstan, kita bisa menggunakan fungsi `lit(nilai_konstan)` dari `pyspark.sql.functions`, yang berfungsi membuat kolom dari nilai literal/konstan.\n","\n","Misalnya kita ingin menambahkan kolom **status** yang bernilai 1"],"metadata":{"id":"1Xu2MZwP0Hab"}},{"cell_type":"code","source":["df.withColumn('status', F.lit(1)).show(5)"],"metadata":{"id":"RfFvAGjqAJ3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Kolom baru dari kolom yang ada\n","\n"],"metadata":{"id":"1LizYZ9dBYzI"}},{"cell_type":"code","source":["df.withColumn('tahun-1', df.year-1).show(5)"],"metadata":{"id":"4rh9fTgaBdsS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Kondisional\n","\n","Untuk menambahkan kolom berdasar beberapa kondisi, gunakan `when` dan `otherwise` (jika perlu).\n","\n","Perhatikan bahwa `when` yang pertama adalah fungsi dalam `pyspark.sql.functions`, sedangkan `when` yang berikutnya adalah fungsi `when` pada object kolom (`pyspark.sql.Column`)\n","\n","Fungsi `otherwise` adalah kondisi *else* atau kondisi selain yang disebutkan pada *when*."],"metadata":{"id":"yaDbu8YSCZqh"}},{"cell_type":"code","source":["df.withColumn('timezone1', F.when(df.timezone == 'WIT', 1).\n","              when(df.timezone == 'WIT', 2).\n","              otherwise(3)).show(5)"],"metadata":{"id":"t70gfGAYBtSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52CxmiOt0dwW"},"source":["###2.6 Data enrichment - Join"]},{"cell_type":"markdown","metadata":{"id":"KO7h6k6iHEXr"},"source":["\n","Perintah ntuk melakukan join adalah sebagai berikut :\n","\n","`df1.join(df2, on=[columname], how=’left’)`\n","\n","Where :\n","-    `df1` − Dataframe1.\n","-    `df2` – Dataframe2.\n","-    `on` − nama kolom yang akan digunakan untuk join.\n","-    `how` – type of join needs to be performed – `left`, `right`, `outer`, `inner`. Defaultnya adalah inner join.\n"]},{"cell_type":"markdown","source":["Create dataframe yang akan digunakan untuk join"],"metadata":{"id":"1kaJk2F5E4WM"}},{"cell_type":"code","metadata":{"id":"3zpT210OGDMa"},"source":["damdata = ((\"SUMATERA SELATAN\",2),\n","(\"SULAWESI TENGAH\",2),\n","(\"SULAWESI SELATAN\",2),\n","(\"SUMATERA BARAT\",3),\n","(\"RIAU\",3),\n","(\"LAMPUNG\",3),\n","(\"NUSA TENGGARA TIMUR\",4),\n","(\"BENGKULU\",8),\n","(\"SUMATERA UTARA\",10),\n","(\"JAWA TIMUR\",12),\n","(\"JAWA TENGAH\",35),\n","(\"JAWA BARAT\",49))\n","\n","df_dam = spark.createDataFrame(damdata).toDF(\"province\", \"dam_num\")\n","\n","df_dam.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8QN1QKE8_Dp9"},"source":["Join dataframe kepadatan penduduk dengan dataframe jumlah bendungan, berdasarkan nama propinsi."]},{"cell_type":"code","metadata":{"id":"4BxH6wBdG7bY"},"source":["df.join(df_dam, on=['province'], how='inner').show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_3UfzMTIDzg"},"source":["Untuk melakukan left join, tentukan parameter `how`"]},{"cell_type":"code","metadata":{"id":"8yLuRcy4IEV3"},"source":["df_2015.join(df_dam, on=['province'], how='left').show(40)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGrlD5ashACP"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Update history\n","\n","2302.1956\n","* _Translate dan lengkapi penjelasan_\n","* _Menambahkan bab transformasi DataFrame_\n","* _Code cleanup_\n"],"metadata":{"id":"1-hOMmiCFsl9"}}]}