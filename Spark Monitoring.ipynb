{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9672ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahun,bulan,jenis,kode_trayek,trayek,jumlah_penumpang\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.88,Terminal Tanjung Priok - Ancol Barat,31197\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.85,Bintara - Cipinang Indah,30711\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.84,Terminal Kampung Melayu - Kapin Raya,40186\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.80,Rawa Buaya - Rawa Kompeni,61883\r",
      "\r\n",
      "2021,10,Mikrotrans,JA.77,Tanjung Priok - Jembatan Item,66616\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.75,Cililitan - Kp. Pulo,50363\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.74,Terminal Rawamangun - Cipinang Muara,50676\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.73,Jambore Cibubur - Pasar Rebo,75846\r",
      "\r\n",
      "2021,10,Mikrotrans,JAK.72,Kampung Rambutan - Pasar Rebo via Poncol,97831\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/hadoop/dataset/penumpang-10-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e440434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b415b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 14:10:14 WARN Utils: Your hostname, dl247-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.15.130 instead (on interface ens33)\n",
      "23/02/15 14:10:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 14:10:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('DataFrame Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d9274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/home/hadoop/dataset/penumpang-10-2021.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434fd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df.groupBy('jenis', 'bulan').agg(avg('jumlah_penumpang'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c01263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------------------+\n",
      "|jenis|bulan|avg(jumlah_penumpang)|\n",
      "+-----+-----+---------------------+\n",
      "|  BRT|   10|    399719.8461538461|\n",
      "+-----+-----+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_group.filter(df_group.jenis=='BRT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f1b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df09 = spark.read.csv('/home/hadoop/dataset/penumpang-09-2021.csv', header=True)\n",
    "df10 = spark.read.csv('/home/hadoop/dataset/penumpang-10-2021.csv', header=True)\n",
    "df11 = spark.read.csv('/home/hadoop/dataset/penumpang-11-2021.csv', header=True)\n",
    "df12 = spark.read.csv('/home/hadoop/dataset/penumpang-12-2021.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1927d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "dflist = [df09, df10, df11, df12]\n",
    "\n",
    "# create merged dataframe\n",
    "df = reduce(pyspark.sql.DataFrame.unionAll, dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf4b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d8218cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+-----------+--------------------+----------------+\n",
      "|tahun|bulan|     jenis|kode_trayek|              trayek|jumlah_penumpang|\n",
      "+-----+-----+----------+-----------+--------------------+----------------+\n",
      "| 2021|   09|Mikrotrans|     JAK.88|Terminal Tanjung ...|           21569|\n",
      "| 2021|   09|Mikrotrans|     JAK.85|Bintara - Cipinan...|           23109|\n",
      "| 2021|   09|Mikrotrans|     JAK.84|Terminal Kampung ...|           36842|\n",
      "| 2021|   09|Mikrotrans|     JAK.80|Rawa Buaya - Rawa...|           46648|\n",
      "| 2021|   09|Mikrotrans|      JA.77|Tanjung Priok - J...|           48302|\n",
      "| 2021|   09|Mikrotrans|     JAK.75|Cililitan - Kp. Pulo|           35365|\n",
      "| 2021|   09|Mikrotrans|     JAK.74|Terminal Rawamang...|           30072|\n",
      "| 2021|   09|Mikrotrans|     JAK.73|Jambore Cibubur -...|           59539|\n",
      "| 2021|   09|Mikrotrans|     JAK.72|Kampung Rambutan ...|           64779|\n",
      "| 2021|   09|Mikrotrans|     JAK.71|Kampung Rambutan ...|           48561|\n",
      "| 2021|   09|Mikrotrans|     JAK.64|Lenteng Agung - A...|           40201|\n",
      "| 2021|   09|Mikrotrans|     JAK.61|Cempaka Putih - P...|           25171|\n",
      "| 2021|   09|Mikrotrans|     JAK.60|Kelapa Gading - R...|           52090|\n",
      "| 2021|   09|Mikrotrans|     JAK.59|Rawamangun - Tana...|           49257|\n",
      "| 2021|   09|Mikrotrans|     JAK.58|Cilingcing - Rorotan|           71733|\n",
      "| 2021|   09|Mikrotrans|     JAK.56|  Grogol - Srengseng|           34100|\n",
      "| 2021|   09|Mikrotrans|     JAK.54|     Grogol - Benhil|           33008|\n",
      "| 2021|   09|Mikrotrans|     JAK.53|Grogol - Pos Peng...|           33250|\n",
      "| 2021|   09|Mikrotrans|     JAK.52|Kalideres - Muara...|           53897|\n",
      "| 2021|   09|Mikrotrans|     JAK.51|Taman Kota - Budi...|           77687|\n",
      "+-----+-----+----------+-----------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bbe131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|jenis                  |\n",
      "+-----------------------+\n",
      "|BRT                    |\n",
      "|Mikrotrans             |\n",
      "|Angkutan Umum Integrasi|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('jenis').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81083fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = spark.createDataFrame([('BRT', 1),\n",
    "                                ('Mikrotrans', 2),\n",
    "                                ('Angkutan Umum Integrasi', 3),\n",
    "                                ('Lain-lain', 4)], [\"type\", \"kode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d740f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+\n",
      "|type                   |kode|\n",
      "+-----------------------+----+\n",
      "|BRT                    |1   |\n",
      "|Mikrotrans             |2   |\n",
      "|Angkutan Umum Integrasi|3   |\n",
      "|Lain-lain              |4   |\n",
      "+-----------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b43d7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df.join(ref,df['jenis'] == ref['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4112ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cast = df_joined.withColumn('jumlah_penumpang',col('jumlah_penumpang').cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04171a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_cast.groupBy('jenis', 'bulan').agg(avg('jumlah_penumpang'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d58f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df_agg.filter(df_agg.jenis=='BRT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c17af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------------------+\n",
      "|jenis|bulan|avg(jumlah_penumpang)|\n",
      "+-----+-----+---------------------+\n",
      "|  BRT|   11|   461483.76923076925|\n",
      "|  BRT|   12|   465652.07692307694|\n",
      "|  BRT|   10|    399719.8461538461|\n",
      "|  BRT|   09|   336068.53846153844|\n",
      "+-----+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80387301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[jenis#84, bulan#83], functions=[avg(jumlah_penumpang#229)])\n",
      "   +- Exchange hashpartitioning(jenis#84, bulan#83, 200), ENSURE_REQUIREMENTS, [plan_id=487]\n",
      "      +- HashAggregate(keys=[jenis#84, bulan#83], functions=[partial_avg(jumlah_penumpang#229)])\n",
      "         +- Project [bulan#83, jenis#84, cast(jumlah_penumpang#87 as int) AS jumlah_penumpang#229]\n",
      "            +- BroadcastHashJoin [jenis#84], [type#209], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=482]\n",
      "               :  +- Union\n",
      "               :     :- Filter (isnotnull(jenis#84) AND (jenis#84 = BRT))\n",
      "               :     :  +- FileScan csv [bulan#83,jenis#84,jumlah_penumpang#87] Batched: false, DataFilters: [isnotnull(jenis#84), (jenis#84 = BRT)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/dataset/penumpang-09-2021.csv], PartitionFilters: [], PushedFilters: [IsNotNull(jenis), EqualTo(jenis,BRT)], ReadSchema: struct<bulan:string,jenis:string,jumlah_penumpang:string>\n",
      "               :     :- Filter (isnotnull(jenis#113) AND (jenis#113 = BRT))\n",
      "               :     :  +- FileScan csv [bulan#112,jenis#113,jumlah_penumpang#116] Batched: false, DataFilters: [isnotnull(jenis#113), (jenis#113 = BRT)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/dataset/penumpang-10-2021.csv], PartitionFilters: [], PushedFilters: [IsNotNull(jenis), EqualTo(jenis,BRT)], ReadSchema: struct<bulan:string,jenis:string,jumlah_penumpang:string>\n",
      "               :     :- Filter (isnotnull(jenis#142) AND (jenis#142 = BRT))\n",
      "               :     :  +- FileScan csv [bulan#141,jenis#142,jumlah_penumpang#145] Batched: false, DataFilters: [isnotnull(jenis#142), (jenis#142 = BRT)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/dataset/penumpang-11-2021.csv], PartitionFilters: [], PushedFilters: [IsNotNull(jenis), EqualTo(jenis,BRT)], ReadSchema: struct<bulan:string,jenis:string,jumlah_penumpang:string>\n",
      "               :     +- Filter (isnotnull(jenis#171) AND (jenis#171 = BRT))\n",
      "               :        +- FileScan csv [bulan#170,jenis#171,jumlah_penumpang#174] Batched: false, DataFilters: [isnotnull(jenis#171), (jenis#171 = BRT)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/dataset/penumpang-12-2021.csv], PartitionFilters: [], PushedFilters: [IsNotNull(jenis), EqualTo(jenis,BRT)], ReadSchema: struct<bulan:string,jenis:string,jumlah_penumpang:string>\n",
      "               +- Project [type#209]\n",
      "                  +- Filter ((type#209 = BRT) AND isnotnull(type#209))\n",
      "                     +- Scan ExistingRDD[type#209,kode#210L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9902cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
